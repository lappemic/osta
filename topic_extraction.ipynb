{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gRqJTU_tqNK6",
        "outputId": "f4973f42-7221-4997-83e3-fcd988efdc6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: yake in /usr/local/lib/python3.10/dist-packages (0.4.8)\n",
            "Requirement already satisfied: multi_rake in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.10/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake) (1.0.3)\n",
            "Requirement already satisfied: pycld2>=0.41 in /usr/local/lib/python3.10/dist-packages (from multi_rake) (0.41)\n",
            "Requirement already satisfied: pyrsistent>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from multi_rake) (0.20.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install nltk pandas yake multi_rake wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "URL = \"https://ost.ecosyste.ms/api/v1/projects?reviewed=true&per_page=3000\"\n",
        "FILE_TO_SAVE_AS = \"projects.json\" # the name you want to save file as\n",
        "\n",
        "\n",
        "resp = requests.get(URL) # making requests to server\n",
        "\n",
        "with open(FILE_TO_SAVE_AS, \"wb\") as f: # opening a file handler to create new file\n",
        "    f.write(resp.content) # writing content to file"
      ],
      "metadata": {
        "id": "5DKM4Sc6szWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz4VKs-fPT1V"
      },
      "outputs": [],
      "source": [
        "raw = pd.read_json(resp.content.decode())\n",
        "print(raw.shape)\n",
        "raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTxhtGjIPT1S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns',200)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "#https://www.nltk.org/\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stpwords = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw)"
      ],
      "metadata": {
        "id": "j4DpieTbuOAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnEQdTsP1bqd"
      },
      "source": [
        "# 1. Goal: Reduce the list of topics below into subtopics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4h6vDKbPT1W"
      },
      "outputs": [],
      "source": [
        "raw['category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw['sub_category'].value_counts()"
      ],
      "metadata": {
        "id": "rwjyvt8Ht_PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAEglWVRPT1W"
      },
      "outputs": [],
      "source": [
        "df = raw[['name','description','category','sub_category','owner','readme']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-roDNj0oF1S"
      },
      "source": [
        "# 2. Sample Row"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import yake"
      ],
      "metadata": {
        "id": "cSh3uz-7vnjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M2gYC_3PT1Z"
      },
      "outputs": [],
      "source": [
        "# Removing words\n",
        "words_black_list = ['python','pvlib','projects','affiliated','http','readthedocs','benchmarks','license','matlab','user','guide','html','https','open','source','journal',\n",
        "                    'latest','release','build','publications','conda','installed','google','documentation','please']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XcqGPN4SDBR"
      },
      "source": [
        "## Yake for keyword Extraction\n",
        "* Using our sample text, we will apply Yake's keyword extraction algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNwC_b_vLROg"
      },
      "source": [
        "## Rake for keyword Extracion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TwHI1iPKy_4"
      },
      "outputs": [],
      "source": [
        "from multi_rake import Rake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ScjShKgSGLx"
      },
      "source": [
        "## KeyBERT for keyword extraction\n",
        "* Using our sample text, we will apply Yake's keyword extraction algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFqx3E5APT1a"
      },
      "outputs": [],
      "source": [
        "#from keybert import KeyBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMyTSgCwPT1b"
      },
      "outputs": [],
      "source": [
        "#kw_model = KeyBERT('distilbert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCzw40KyZcwp"
      },
      "source": [
        "# 3. Cleaning Full Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLs93IGxYCs_"
      },
      "outputs": [],
      "source": [
        "# Fill null values with 'None'. This will allow us to concatenate the relevant text columns\n",
        "df = df.fillna('None')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69dPuRrQaHuC"
      },
      "outputs": [],
      "source": [
        "# Combine text columns\n",
        "combined = df['description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCI6xTP5aZmv"
      },
      "outputs": [],
      "source": [
        "df['combined'] = combined\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OXAWga1a8Vv"
      },
      "outputs": [],
      "source": [
        "# Words to remove from column. We will keep adding to this list the more words we find that are irrelavent\n",
        "words_black_list = ['python','projects','affiliated','http','readthedocs','benchmarks',\n",
        "                    'license','matlab','user','guide','html','https','open','source','journal',\n",
        "                    'latest','release','build','publications','conda','installed','users','using'\n",
        "                    'google','documentation','please','github','data','model','install','code',\n",
        "                    'package','badge','project']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# More words to remove\n",
        "words_black_list = ['python','tool','based','pvlib','projects','affiliated','http','readthedocs','benchmarks','aims','high','series','functions','large','calculate','applications','flow','easy','contains','state','collection','real','flexsible','program','implementation','evaluate','track','engineering',\n",
        "                    'license','matlab','user','guide','html','https','open','source','journal','various','written','toolbox','working','design','related','level','deep','standard','within','applications','methods','toolkits','multi','network','coupled','platform','application','developed','possible',\n",
        "                    'latest','release','build','publications','conda','installed','users','using','julia','different','toolkit','toolkit','provide','focus','smart','simple','enables','sources','client','advanced','digital','across','suite','domain','estimate','evaluation','measurements','statistical',\n",
        "                    'google','documentation','please','github','data','model','install','code','numerical','simulation','algorithms','driven','interactive','helps', 'package','resolution','visualization','images','explore','develop','analysis','distributed','calculations','global','control','accessible',\n",
        "                    'package','badge','project','version','file','view','system','master','used','also','providing','enable','multiple','point','image','process','including','components','computer','easily','built','range','line','calculations','term','uses','faciliate','online','study','evaluate',\n",
        "                    'repository','example','docs','files','information','models','software','available','work','making','visualize','format','load','usage','module','apis','automated','temporal','curated','variable','best','variables','websites','sets','near','worldwide','automatic','problems','custom',\n",
        "                    'zenodo','command','modis','pull','request','pctl','theoj','brodiepearson','blob','getting','order','fast','daily','makes','domaine','around','processes','associated','long','type','clean','optimal','many','awesome','extract','three','variety','useful','inventory','allowing',\n",
        "                    'started','machine','learning','make','sure','pypi','ipcc','main','scholor','colorado','codecov','individual','researchers','historical','metadata','supporting','finite','analyze','databases','chemical','initiative','technologies','collect','hourly','given','solver','space',\n",
        "                    'none','jupyter','notebook','united','states','docker','anaconda','forge','datasets','false','would','reference','tracking','identify','general','techniques','scripts','world','libraries','estimating','neutral','response','algorithm','ground','retrieving','retrieve','quantitative',\n",
        "                    'like','gustavoirgang','google','collab','sciencedirect','yaml','downloads','actions','workflows', 'understanding','international','compute','gridded','studies','single','calculates','exchange','potential','robust','dimensional','reporting','development','simulations','flexsible',\n",
        "                    'media','icon','joss','papers','legend','description','jobs','download','input','output','latitude','longitude','analyse','measurement','directly','official','sheet','classifications','find','features','computational','operations','grids','includes','text','standardized','dedicated','easier','purpose',\n",
        "                    'unit','tests','cran','nbsp','colab','check','plot','ncss','contributing','installation', 'instructions','fortran','complex','simulating','component','access','bottom','flexible','search','agent','results','needed','creating','friendly','basic','linear','assess','wrapper','defined','wide','method',\n",
        "                    'wiki','wikipedia','feel','free','name','list','issues','start','examples','index','dataset','branch','create','high','performance','management','scale','website','codes','programs','downnloading','utility','imagery','events','websites','share','solutions','published',\n",
        "                    'enabling','specification','semantic','inputs','query','vertical','estimates','freely','several','object','specific','parameters','discover','accelerate','calculating','functionality','cover','testing','operation','reinforcement','form','perform',\n",
        "                    'better','towards','layer','evaluating','speed','comprehensive','cells','energyplus','derived','interactions','layers','emis','emerging','apps','hardware','past','read','android','utilities','nrel','better','repositories','simulate','oriented','european','tastes',\n",
        "                    'availability','part','language','extent','europe','german','germany','quickly','approach','total','programming','short','visualise','produced','intended','quantifying','availability','plugin',\n",
        "                    'server','noaa','existing','volume','primary','canada','post','core','carlo','platforms','gtfs','visualizations','chain','independent','benchmark','modules',\n",
        "                    'options','usgs','australian','commercial','monte','capable','neutral','save','generate','manage','factors','modelica','rapid','boundary','relevant','simulates','powerful','stakeholders','dynamics',\n",
        "                    'containing','aware','operators','reading','notebooks','calculator','visualizing','statistics','direction','ensemble','automate','edge','xarray','integrating','along',\n",
        "                    'modeling','national','quality','modelling','service','modular','neutral','small','australia','cell','british','columbia','generating','analyses','sharing','notebook','experiments','portal','types','quantify','artificial','focused','protocol','highly','equations','first','tasks','arduino','suitable','automation','compare','comparison',\n",
        "                    'library','following','test','running','import','database','access','packages','directory','need','tools','simulation','time','framework','systems','designed','community','interface','optimization','allows','support','grid','research','processing','help','provides']"
      ],
      "metadata": {
        "id": "ZoBvEVbz0ezW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_black_list)"
      ],
      "metadata": {
        "id": "aKWZZ3jdE4Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAX050Xnanob"
      },
      "outputs": [],
      "source": [
        "# Create new column called \"cleaned_text\" and apply different cleaning methods\n",
        "df['cleaned_text'] = df['combined'].apply(lambda x: x.replace(\"\\\\n\", \" \")) # Replace \\\\n\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x.lower() for x in str(x).split())) # Lowercase\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'<[^>]+>', ' ', x)) # Remove HTML tags\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]',' ', x)) # Remove symbols\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if len(x) > 3)) # Remove short words\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if x.isalpha())) # Remove numbers\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if x not in words_black_list)) # Remove words from blacklist\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if x not in stpwords)) # Remove stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gqZo79tboxy"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHbJ_08IdVz-"
      },
      "source": [
        "# 4. Unique Words\n",
        "\n",
        "* Let's get a look at the most common words in our cleaned_text column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-rg0PaTdffl"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Tokenize text\n",
        "tokenized_data = [nltk.word_tokenize(text) for text in df['cleaned_text']]\n",
        "\n",
        "# count the occurrence of each token\n",
        "token_counts = [Counter(tokens) for tokens in tokenized_data]\n",
        "\n",
        "# combine the counts from all rows\n",
        "combined_counts = sum(token_counts, Counter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDC0AWZrdn8n"
      },
      "outputs": [],
      "source": [
        "# print the top 30 most common tokens\n",
        "word_freq = combined_counts.most_common(400)\n",
        "word_freq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 120"
      ],
      "metadata": {
        "id": "LVLNZX7l7X-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc = WordCloud(background_color=\"white\", max_words=500,width=2000,height=2000,relative_scaling=0.5)\n",
        "wc.generate_from_frequencies(dict(word_freq))\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.clf()\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ALtUzHN78zkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jc2cfWzjWEA"
      },
      "source": [
        "# 5. Unique Bigrams and Trigrams\n",
        "\n",
        "* We should also look at the most common bigrams and trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhWlnFs0iLWV"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text'].str.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryevOv3Gi7ql"
      },
      "outputs": [],
      "source": [
        "# Create list of words in the order in which they orignally appear\n",
        "words = list(df['cleaned_text'].str.split().apply(pd.Series).stack().reset_index(drop = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siEdV5eHvh2D"
      },
      "outputs": [],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu9vfANLe2EA"
      },
      "outputs": [],
      "source": [
        "# Most frequent bigrams\n",
        "bigrams = (pd.Series(nltk.ngrams(words, 2)).value_counts()).to_frame('count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA8CjN3iySop"
      },
      "outputs": [],
      "source": [
        "bigrams.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH01om74c683"
      },
      "outputs": [],
      "source": [
        "# Most frequent trigrams\n",
        "(pd.Series(nltk.ngrams(words, 3)).value_counts())[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWWPCx16dYso"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpH2FE251IXG"
      },
      "source": [
        "#6. Further Work\n",
        "\n",
        "* Further reduce the number of unique keywords by removing irrelevant words\n",
        "* Create a list of topics/subtopics that we want\n",
        "* Define features (words, bigrams, trigrams)\n",
        "* cosine similarity to get similar projects\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8 (XPython)",
      "language": "python",
      "name": "xpython"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}