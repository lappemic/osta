{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/protontypes/AwesomeCure/main/csv/projects_with_readme.csv\n",
    "!pip install nltk pandas yake multi_rake keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTxhtGjIPT1S"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',200)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stpwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "Qz4VKs-fPT1V",
    "outputId": "f19268b8-95bb-43ff-a8ac-61fa0d1f6355"
   },
   "outputs": [],
   "source": [
    "raw = pd.read_csv('projects_with_readme.csv')\n",
    "print(raw.shape)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnEQdTsP1bqd"
   },
   "source": [
    "# 1. Goal: Reduce the list of topics below into subtopics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4h6vDKbPT1W",
    "outputId": "fcff9d82-49b5-4dee-bb91-b736a0b56725"
   },
   "outputs": [],
   "source": [
    "raw['rubric'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JAEglWVRPT1W",
    "outputId": "dc6becb8-d0e8-446c-8f06-2c481032ec20"
   },
   "outputs": [],
   "source": [
    "df = raw[['project_name','oneliner','rubric','topics','git_namespace','readme_content']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-roDNj0oF1S"
   },
   "source": [
    "# 2. Sample Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIKqrwfBPT1X",
    "outputId": "c90bfc2d-f661-4de2-c246-7a5435c64800"
   },
   "outputs": [],
   "source": [
    "# Let's get a look at the relevant columns\n",
    "print(df['rubric'][0])\n",
    "print(df['oneliner'][0])\n",
    "print(df['topics'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0EHdl-hYvAb"
   },
   "outputs": [],
   "source": [
    "# Save the columns for the first row into relevant variables for testing\n",
    "oneliner = df['oneliner'][0]\n",
    "topics = df['topics'][0]\n",
    "readme = df['readme_content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "2iHZGEouPT1Y",
    "outputId": "aeee0449-761c-4f10-fa5d-8b64b2f9ba0f"
   },
   "outputs": [],
   "source": [
    "# Combining oneliner, topics, and readme to get the most keywords as possible\n",
    "sample = oneliner +' '+ topics +' '+ readme\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42UJgQT8PT1Z"
   },
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "import re\n",
    "clean_sample1 = sample.replace('\\\\n',' ') # Remove \\\\n\n",
    "clean_sample2 = re.sub(r'<[^>]+>', ' ', clean_sample1)  # Remove HTML tags\n",
    "clean_sample3 = re.sub(r'[^a-zA-Z0-9\\s]', ' ', clean_sample2)  # Remove non-alphanumeric characters except spaces\n",
    "clean_sample3 = clean_sample3.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "6M2gYC_3PT1Z",
    "outputId": "3f193d22-d428-4b93-b27e-554dfce9b0ce"
   },
   "outputs": [],
   "source": [
    "# Removing words\n",
    "words_black_list = ['python','pvlib','projects','affiliated','http','readthedocs','benchmarks','license','matlab','user','guide','html','https','open','source','journal',\n",
    "                    'latest','release','build','publications','conda','installed','google','documentation','please']\n",
    "\n",
    "words_list = clean_sample3.split() # Turn into a list\n",
    "words_list2 = [x for x in words_list if len(x) > 3] # Remove short words\n",
    "words_list3 = [x for x in words_list2 if x.isalpha()] # Remove numbers\n",
    "words_list4 = [x for x in words_list3 if x not in words_black_list] # Remove blacklisted words\n",
    "words_list5 = [x for x in words_list4 if not x in stpwords] # Remove stopwords\n",
    "words_string = ' '.join(words_list5) # Turn back into string\n",
    "words_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XcqGPN4SDBR"
   },
   "source": [
    "## Yake for keyword Extraction\n",
    "* Using our sample text, we will apply Yake's keyword extraction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7m9grarPT1a",
    "outputId": "d68c1741-c917-4b58-db6f-2a76bd4642b2"
   },
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(top=20, stopwords='en')\n",
    "keywords = kw_extractor.extract_keywords(words_string)\n",
    "for kw, v in keywords:\n",
    "  print(\"Keyphrase: \",kw, \": score\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNwC_b_vLROg"
   },
   "source": [
    "## Rake for keyword Extracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TwHI1iPKy_4",
    "outputId": "f10f7453-12b2-4280-9ec3-d299fe1845a3"
   },
   "outputs": [],
   "source": [
    "from multi_rake import Rake\n",
    "rake = Rake()\n",
    "keywords = rake.apply(words_string)\n",
    "print(keywords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ScjShKgSGLx"
   },
   "source": [
    "## KeyBERT for keyword extraction\n",
    "* Using our sample text, we will apply Yake's keyword extraction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOb0euKgZHHr"
   },
   "outputs": [],
   "source": [
    "pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFqx3E5APT1a"
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMyTSgCwPT1b"
   },
   "outputs": [],
   "source": [
    "kw_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "bert_keywords = kw_model.extract_keywords(words_string,keyphrase_ngram_range=(3,3), stop_words='english',\n",
    "                                          use_mmr=True,diversity=0.9, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ya4hEj1gPT1c",
    "outputId": "54f5d8de-a03f-4a04-9974-dba22eddf832"
   },
   "outputs": [],
   "source": [
    "bert_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCzw40KyZcwp"
   },
   "source": [
    "# 3. Cleaning Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLs93IGxYCs_"
   },
   "outputs": [],
   "source": [
    "# Fill null values with 'None'. This will allow us to concatenate the relevant text columns\n",
    "df = df.fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69dPuRrQaHuC"
   },
   "outputs": [],
   "source": [
    "# Combine text columns\n",
    "combined = df['oneliner'] + ' '+ df['topics'] + ' '+ df['readme_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "hCI6xTP5aZmv",
    "outputId": "36e0ad63-64c9-496e-adf5-5bcf9ed67909"
   },
   "outputs": [],
   "source": [
    "df['combined'] = combined\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OXAWga1a8Vv"
   },
   "outputs": [],
   "source": [
    "# Words to remove from column. We will keep adding to this list the more words we find that are irrelavent\n",
    "words_black_list = ['python','pvlib','projects','affiliated','http','readthedocs','benchmarks',\n",
    "                    'license','matlab','user','guide','html','https','open','source','journal',\n",
    "                    'latest','release','build','publications','conda','installed','users','using'\n",
    "                    'google','documentation','please','github','data','model','install','code',\n",
    "                    'package','badge','project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAX050Xnanob"
   },
   "outputs": [],
   "source": [
    "# Create new column called \"cleaned_text\" and apply different cleaning methods\n",
    "df['cleaned_text'] = df['combined'].apply(lambda x: x.replace(\"\\\\n\", \" \")) # Replace \\\\n\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x.lower() for x in str(x).split())) # Lowercase\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'<[^>]+>', ' ', x)) # Remove HTML tags\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]',' ', x)) # Remove symbols\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if len(x) > 3)) # Remove short words\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if x.isalpha())) # Remove numbers\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if x not in words_black_list)) # Remove words from blacklist\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if x not in stpwords)) # Remove stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gqZo79tboxy",
    "outputId": "9ea933d3-e517-4768-e143-df01b29c17d4"
   },
   "outputs": [],
   "source": [
    "df['cleaned_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHbJ_08IdVz-"
   },
   "source": [
    "# 4. Unique Words\n",
    "\n",
    "* Let's get a look at the most common words in our cleaned_text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-rg0PaTdffl"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize text\n",
    "tokenized_data = [nltk.word_tokenize(text) for text in df['cleaned_text']]\n",
    "\n",
    "# count the occurrence of each token\n",
    "token_counts = [Counter(tokens) for tokens in tokenized_data]\n",
    "\n",
    "# combine the counts from all rows\n",
    "combined_counts = sum(token_counts, Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDC0AWZrdn8n",
    "outputId": "20820b09-c758-4e19-997f-06634a9ecaf5"
   },
   "outputs": [],
   "source": [
    "# print the top 30 most common tokens\n",
    "combined_counts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq8wphh5e2Lt"
   },
   "outputs": [],
   "source": [
    "# More words to remove\n",
    "words_black_list = ['python','pvlib','projects','affiliated','http','readthedocs','benchmarks',\n",
    "                    'license','matlab','user','guide','html','https','open','source','journal',\n",
    "                    'latest','release','build','publications','conda','installed','users','using',\n",
    "                    'google','documentation','please','github','data','model','install','code',\n",
    "                    'package','badge','project','version','file','view','system','master','used','also',\n",
    "                    'repository','example','docs','files','information','models','software','available',\n",
    "                    'zenodo','command','modis','pull','request','pctl','theoj','brodiepearson','blob','getting',\n",
    "                    'started','machine','learning','make','sure','pypi','ipcc','main','scholor','colorado','codecov',\n",
    "                    'none','jupyter','notebook','united','states','docker','anaconda','forge','datasets','false','would',\n",
    "                    'like','gustavoirgang','google','collab','sciencedirect','yaml','downloads','actions','workflows',\n",
    "                    'media','icon','joss','papers','legend','description','jobs','download','input','output','latitude','longitude',\n",
    "                    'unit','tests','cran','nbsp','colab','check','plot','ncss','contributing','installation', 'instructions',\n",
    "                    'wiki','wikipedia','feel','free','name','list','issues','start','examples','index','dataset','branch','create',\n",
    "                    'library','following','test','running','import','database','access','packages','directory','need','change','tools']\n",
    "\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x for x in str(x).split() if x not in words_black_list)) # Remove words from blacklist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jc2cfWzjWEA"
   },
   "source": [
    "# 5. Unique Bigrams and Trigrams\n",
    "\n",
    "* We should also look at the most common bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhWlnFs0iLWV",
    "outputId": "51c16b92-c00d-40bf-8ecc-5cd7b23486f2"
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryevOv3Gi7ql"
   },
   "outputs": [],
   "source": [
    "# Create list of words in the order in which they orignally appear\n",
    "words = list(df['cleaned_text'].str.split().apply(pd.Series).stack().reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "siEdV5eHvh2D",
    "outputId": "7c8c1ddc-de25-4f3a-a057-f97744d976a1"
   },
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu9vfANLe2EA"
   },
   "outputs": [],
   "source": [
    "# Most frequent bigrams\n",
    "bigrams = (pd.Series(nltk.ngrams(words, 2)).value_counts()).to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "uA8CjN3iySop",
    "outputId": "c407cbc7-7144-4daf-fdb6-a4c03d9f7c44"
   },
   "outputs": [],
   "source": [
    "bigrams.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH01om74c683",
    "outputId": "d575b187-9d94-4e52-d8b5-d5d262ee505a"
   },
   "outputs": [],
   "source": [
    "# Most frequent trigrams\n",
    "(pd.Series(nltk.ngrams(words, 3)).value_counts())[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWWPCx16dYso"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpH2FE251IXG"
   },
   "source": [
    "#6. Further Work\n",
    "\n",
    "* Further reduce the number of unique keywords by removing irrelevant words\n",
    "* Create a list of topics/subtopics that we want\n",
    "* Define features (words, bigrams, trigrams)\n",
    "* cosine similarity to get similar projects\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
